import dashscope
import requests
import re
from typing import Optional, Dict
from database import settings
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize DashScope API
dashscope.api_key = settings.DASHSCOPE_API_KEY

# Initialize OpenAI client for NVIDIA API (if needed)
_nvidia_client = None

def get_nvidia_client():
    """Get or create NVIDIA OpenAI client"""
    global _nvidia_client
    if _nvidia_client is None and settings.NVIDIA_API_KEY:
        try:
            from openai import OpenAI
            _nvidia_client = OpenAI(
                base_url="https://integrate.api.nvidia.com/v1",
                api_key=settings.NVIDIA_API_KEY
            )
        except ImportError:
            logger.error("OpenAI library not installed. Please run: pip install openai")
            return None
    return _nvidia_client


class NewsSummarizer:
    """Generate news summaries using Alibaba Qwen LLM or Local Ollama"""
    
    def __init__(self):
        self.provider = settings.LLM_PROVIDER
        if self.provider == "ollama":
            self.model = settings.OLLAMA_MODEL
            self.base_url = settings.OLLAMA_BASE_URL.rstrip('/')
            self.api_url = f"{self.base_url}/api/chat"
            # åˆå§‹åŒ–æ—¶æ£€æŸ¥è¿æ¥
            self._check_ollama_connection()
        elif self.provider == "nvidia":
            self.model = settings.NVIDIA_MODEL
            # æ£€æŸ¥NVIDIA API key
            if not settings.NVIDIA_API_KEY or settings.NVIDIA_API_KEY == "":
                logger.warning("NVIDIA API key not configured")
            else:
                client = get_nvidia_client()
                if client:
                    logger.info(f"NVIDIA GLM API initialized, model: {self.model}")
                else:
                    logger.error("Failed to initialize NVIDIA client")
        else:
            self.model = "qwen-turbo"  # Low-cost model, can upgrade to "qwen-plus"
    
    def _check_ollama_connection(self):
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            # æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ
            health_url = f"{self.base_url}/api/tags"
            response = requests.get(health_url, timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [m.get("name", "") for m in models]
                if self.model not in model_names:
                    logger.warning(
                        f"Ollamaæ¨¡å‹ '{self.model}' æœªæ‰¾åˆ°ã€‚å¯ç”¨æ¨¡å‹: {', '.join(model_names)}"
                    )
                else:
                    logger.info(f"Ollamaè¿æ¥æ­£å¸¸ï¼Œä½¿ç”¨æ¨¡å‹: {self.model}")
            else:
                logger.warning(f"OllamaæœåŠ¡å“åº”å¼‚å¸¸: HTTP {response.status_code}")
        except requests.exceptions.ConnectionError:
            logger.error(
                f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ ({self.base_url})ã€‚"
                f"è¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ: ollama serve"
            )
        except requests.exceptions.Timeout:
            logger.error(f"è¿æ¥OllamaæœåŠ¡è¶…æ—¶ ({self.base_url})")
        except Exception as e:
            logger.warning(f"æ£€æŸ¥Ollamaè¿æ¥æ—¶å‡ºé”™: {str(e)}")
    
    def generate_summary(
        self, 
        title: str, 
        content: str, 
        roast_mode: bool = False
    ) -> str:
        """
        Generate a concise 1-2 sentence summary of news article
        
        Args:
            title: News title
            content: News content/description
            roast_mode: If True, generate humorous/roast-style summary
        
        Returns:
            Summary string
        """
        if self.provider == "dashscope":
            return self._generate_dashscope(title, content, roast_mode)
        elif self.provider == "ollama":
            return self._generate_ollama(title, content, roast_mode)
        elif self.provider == "nvidia":
            return self._generate_nvidia(title, content, roast_mode)
        else:
            logger.warning(f"Unknown LLM provider: {self.provider}, using fallback")
            return self._fallback_summary(title, content, roast_mode)

    def _generate_ollama(self, title: str, content: str, roast_mode: bool) -> str:
        """Generate summary using local Ollama model"""
        try:
            prompt = self._build_prompt(title, content, roast_mode)
            
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "stream": False,
                "temperature": 0.8 if roast_mode else 0.3,
            }
            
            response = requests.post(self.api_url, json=payload, timeout=120)
            
            if response.status_code == 200:
                result = response.json()
                if "message" in result and "content" in result["message"]:
                    summary = result["message"]["content"].strip()
                    logger.info(f"Generated summary (Ollama) for: {title[:50]}...")
                    return summary
                elif "choices" in result and len(result["choices"]) > 0:
                    # Fallback to OpenAI-compatible format if available
                    summary = result["choices"][0]["message"]["content"].strip()
                    logger.info(f"Generated summary (Ollama OpenAI-compat) for: {title[:50]}...")
                    return summary
                else:
                    logger.error(f"Ollamaå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                    return self._fallback_summary(title, content, roast_mode)
            else:
                error_msg = f"Ollama APIé”™è¯¯: HTTP {response.status_code}"
                try:
                    error_detail = response.json()
                    error_msg += f" - {error_detail}"
                except:
                    error_msg += f" - {response.text[:200]}"
                logger.error(error_msg)
                return self._fallback_summary(title, content, roast_mode)
            
        except requests.exceptions.ConnectionError as e:
            logger.error(
                f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ ({self.api_url})ã€‚"
                f"è¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ: ollama serve"
            )
            return self._fallback_summary(title, content, roast_mode)
        except requests.exceptions.Timeout:
            logger.error(f"Ollamaè¯·æ±‚è¶…æ—¶ (è¶…è¿‡120ç§’)ï¼Œä½¿ç”¨å¤‡ç”¨æ‘˜è¦")
            return self._fallback_summary(title, content, roast_mode)
        except requests.exceptions.RequestException as e:
            logger.error(f"Ollamaè¯·æ±‚å¼‚å¸¸: {str(e)}")
            return self._fallback_summary(title, content, roast_mode)
        except Exception as e:
            logger.error(f"Ollamaæ‘˜è¦ç”Ÿæˆé”™è¯¯: {str(e)}", exc_info=True)
            return self._fallback_summary(title, content, roast_mode)

    def _generate_nvidia(self, title: str, content: str, roast_mode: bool) -> str:
        """Generate summary using NVIDIA GLM API"""
        if not settings.NVIDIA_API_KEY or settings.NVIDIA_API_KEY == "":
            logger.warning("NVIDIA API key not configured, using fallback summary")
            return self._fallback_summary(title, content, roast_mode)
        
        try:
            client = get_nvidia_client()
            if not client:
                logger.error("NVIDIA client not available")
                return self._fallback_summary(title, content, roast_mode)
            
            # æ„å»ºç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯
            if roast_mode:
                system_prompt = "ä½ æ˜¯èªæ˜ã€å¹½é»˜ã€æœ‰ç‚¹æ¯’èˆŒçš„æ–°é—»è¯„è®ºå‘˜ï¼Œæ“…é•¿ç”¨ä¿çš®ã€æç¬‘ã€ç•¥å¸¦åæ§½çš„è¯­æ°”æ€»ç»“æ–°é—»ã€‚"
                user_prompt = f"""æ–°é—»æ ‡é¢˜ï¼š{title}

æ–°é—»å†…å®¹ï¼š{content}

è¯·ç”¨1-2å¥è¯æ€»ç»“è¿™æ¡æ–°é—»ï¼Œè¦æ±‚ï¼š
1. è¯­æ°”å¹½é»˜ã€ä¿çš®ï¼Œå¯ä»¥é€‚å½“è°ƒä¾ƒ
2. æŠ“ä½æ–°é—»æ ¸å¿ƒè¦ç‚¹
3. åŠ å…¥ä¸€äº›ç½‘ç»œæµè¡Œè¯­æˆ–æ®µå­é£æ ¼
4. ä¿æŒç®€æ´ï¼Œä¸è¶…è¿‡60å­—"""
            else:
                system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»æ‘˜è¦åŠ©æ‰‹ï¼Œæ“…é•¿ç”¨ç®€æ´ã€å®¢è§‚çš„è¯­è¨€æ€»ç»“æ–°é—»è¦ç‚¹ã€‚"
                user_prompt = f"""æ–°é—»æ ‡é¢˜ï¼š{title}

æ–°é—»å†…å®¹ï¼š{content}

è¯·ç”¨1-2å¥è¯æ€»ç»“è¿™æ¡æ–°é—»çš„æ ¸å¿ƒå†…å®¹ï¼Œè¦æ±‚ï¼š
1. å®¢è§‚ä¸­æ€§ï¼Œä¸å¸¦ä¸ªäººæƒ…æ„Ÿ
2. å‡†ç¡®æç‚¼å…³é”®ä¿¡æ¯
3. è¯­è¨€ç®€æ´ä¸“ä¸š
4. ä¸è¶…è¿‡50å­—"""
            
            # è°ƒç”¨NVIDIA API
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.8 if roast_mode else 0.3,
                max_tokens=800,  # å¢åŠ åˆ°800ï¼Œé¿å…æˆªæ–­
                stream=False
            )
            
            # æå–æ‘˜è¦
            if response.choices and len(response.choices) > 0:
                message = response.choices[0].message
                choice = response.choices[0]
                
                # ä¼˜å…ˆä½¿ç”¨contentå­—æ®µ
                if message and message.content:
                    summary = message.content.strip()
                    if summary:
                        logger.info(f"Generated summary (NVIDIA GLM) for: {title[:50]}...")
                        return summary
                
                # å¦‚æœcontentä¸ºNoneï¼Œæ£€æŸ¥reasoning_contentå­—æ®µï¼ˆGLMæ¨ç†æ¨¡å¼ï¼‰
                if hasattr(message, 'reasoning_content') and message.reasoning_content:
                    reasoning = message.reasoning_content.strip()
                    logger.info(f"Using reasoning_content from NVIDIA GLM for: {title[:50]}...")
                    # å°è¯•ä»æ¨ç†å†…å®¹ä¸­æå–æœ€ç»ˆç­”æ¡ˆ
                    # GLMæ¨ç†æ¨¡å¼ä¼šåœ¨reasoning_contentä¸­åŒ…å«æœ€ç»ˆç­”æ¡ˆ
                    if reasoning:
                        # æ–¹æ³•1ï¼šæŸ¥æ‰¾å¼•å·ä¸­çš„å†…å®¹ï¼ˆå¯èƒ½æ˜¯æœ€ç»ˆç­”æ¡ˆï¼‰
                        quoted = re.findall(r'["\']([^"\']+)["\']', reasoning)
                        if quoted:
                            # å–æœ€åä¸€ä¸ªå¼•å·å†…å®¹ï¼ˆé€šå¸¸æ˜¯æœ€ç»ˆç­”æ¡ˆï¼‰
                            summary = quoted[-1].strip()
                            if summary and len(summary) < 200:  # åˆç†çš„é•¿åº¦
                                return summary
                        
                        # æ–¹æ³•2ï¼šæŸ¥æ‰¾æœ€åä¸€æ®µä»¥å¼•å·å¼€å¤´çš„å†…å®¹
                        lines = reasoning.split('\n')
                        for line in reversed(lines):
                            line = line.strip()
                            if line and (line.startswith('"') or line.startswith("'")):
                                # æå–å¼•å·å†…å®¹
                                match = re.search(r'["\']([^"\']+)["\']', line)
                                if match:
                                    summary = match.group(1).strip()
                                    if summary and len(summary) < 200:
                                        return summary
                        
                        # æ–¹æ³•3ï¼šå¦‚æœæ‰¾ä¸åˆ°ï¼Œå–æœ€åä¸€æ®µéç©ºè¡Œï¼ˆå»é™¤éå†…å®¹éƒ¨åˆ†ï¼‰
                        last_paragraph = ""
                        for line in reversed(lines):
                            line = line.strip()
                            if line and not line.startswith('*') and not line.startswith('6.'):
                                if '**' not in line:  # è·³è¿‡æ ‡é¢˜è¡Œ
                                    last_paragraph = line
                                    break
                        
                        if last_paragraph:
                            # æå–å¼•å·å†…å®¹æˆ–ç›´æ¥ä½¿ç”¨
                            match = re.search(r'["\']([^"\']+)["\']', last_paragraph)
                            if match:
                                return match.group(1).strip()
                            return last_paragraph[:150]  # é™åˆ¶é•¿åº¦
                        
                        # æ–¹æ³•4ï¼šå¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›å‰200å­—ç¬¦
                        return reasoning[:200]
                
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè®°å½•è­¦å‘Š
                finish_reason = choice.finish_reason if hasattr(choice, 'finish_reason') else 'unknown'
                logger.warning(f"NVIDIA API returned no content. Finish reason: {finish_reason}")
                return self._fallback_summary(title, content, roast_mode)
            else:
                logger.error("NVIDIA API returned empty choices")
                return self._fallback_summary(title, content, roast_mode)
                
        except Exception as e:
            logger.error(f"NVIDIA GLM API error: {str(e)}", exc_info=True)
            return self._fallback_summary(title, content, roast_mode)
    
    def _generate_dashscope(self, title: str, content: str, roast_mode: bool) -> str:
        """Generate summary using DashScope (Alibaba Cloud)"""
        if not settings.DASHSCOPE_API_KEY or settings.DASHSCOPE_API_KEY == "":
            logger.warning("DashScope API key not configured, using fallback summary")
            return self._fallback_summary(title, content, roast_mode)
        
        try:
            prompt = self._build_prompt(title, content, roast_mode)
            
            response = dashscope.Generation.call(
                model=self.model,
                prompt=prompt,
                max_tokens=150,
                temperature=0.8 if roast_mode else 0.3,
                top_p=0.9
            )
            
            if response.status_code == 200:
                summary = response.output.text.strip()
                logger.info(f"Generated summary (DashScope) for: {title[:50]}...")
                return summary
            else:
                logger.error(f"DashScope API error: {response.message}")
                return self._fallback_summary(title, content, roast_mode)
                
        except Exception as e:
            logger.error(f"Summary generation error: {str(e)}")
            return self._fallback_summary(title, content, roast_mode)
    
    def _build_prompt(self, title: str, content: str, roast_mode: bool) -> str:
        """Build prompt for LLM based on mode"""
        
        if roast_mode:
            return f"""ä½ æ˜¯ä¸€ä¸ªå¹½é»˜é£è¶£çš„æ–°é—»è¯„è®ºå‘˜ï¼Œæ“…é•¿ç”¨ä¿çš®ã€æç¬‘ã€ç•¥å¸¦åæ§½çš„è¯­æ°”æ€»ç»“æ–°é—»ã€‚

æ–°é—»æ ‡é¢˜ï¼š{title}
æ–°é—»å†…å®¹ï¼š{content}

è¯·ç”¨1-2å¥è¯æ€»ç»“è¿™æ¡æ–°é—»ï¼Œè¦æ±‚ï¼š
1. è¯­æ°”å¹½é»˜ã€ä¿çš®ï¼Œå¯ä»¥é€‚å½“è°ƒä¾ƒ
2. æŠ“ä½æ–°é—»æ ¸å¿ƒè¦ç‚¹
3. åŠ å…¥ä¸€äº›ç½‘ç»œæµè¡Œè¯­æˆ–æ®µå­é£æ ¼
4. ä¿æŒç®€æ´ï¼Œä¸è¶…è¿‡60å­—

åæ§½å¼æ‘˜è¦ï¼š"""
        else:
            return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»æ‘˜è¦åŠ©æ‰‹ï¼Œæ“…é•¿ç”¨ç®€æ´ã€å®¢è§‚çš„è¯­è¨€æ€»ç»“æ–°é—»è¦ç‚¹ã€‚

æ–°é—»æ ‡é¢˜ï¼š{title}
æ–°é—»å†…å®¹ï¼š{content}

è¯·ç”¨1-2å¥è¯æ€»ç»“è¿™æ¡æ–°é—»çš„æ ¸å¿ƒå†…å®¹ï¼Œè¦æ±‚ï¼š
1. å®¢è§‚ä¸­æ€§ï¼Œä¸å¸¦ä¸ªäººæƒ…æ„Ÿ
2. å‡†ç¡®æç‚¼å…³é”®ä¿¡æ¯
3. è¯­è¨€ç®€æ´ä¸“ä¸š
4. ä¸è¶…è¿‡50å­—

æ‘˜è¦ï¼š"""
    
    def evaluate_relevance(self, topic: str, title: str, content: str) -> float:
        """
        è¯„ä¼°æ–°é—»ä¸ä¸»é¢˜çš„ç›¸å…³æ€§åˆ†æ•° (0-1)
        
        Args:
            topic: ä¸»é¢˜åç§°
            title: æ–°é—»æ ‡é¢˜
            content: æ–°é—»å†…å®¹
            
        Returns:
            float: ç›¸å…³æ€§åˆ†æ•° (0-1)ï¼Œé»˜è®¤0.5
        """
        try:
            if self.provider == "nvidia":
                return self._evaluate_relevance_nvidia(topic, title, content)
            elif self.provider == "ollama":
                return self._evaluate_relevance_ollama(topic, title, content)
            elif self.provider == "dashscope":
                return self._evaluate_relevance_dashscope(topic, title, content)
            else:
                logger.warning(f"Unknown LLM provider for relevance evaluation: {self.provider}")
                return 0.5  # é»˜è®¤åˆ†æ•°
        except Exception as e:
            logger.error(f"Error evaluating relevance: {str(e)}", exc_info=True)
            return 0.5  # å‡ºé”™æ—¶è¿”å›é»˜è®¤åˆ†æ•°
    
    def _evaluate_relevance_nvidia(self, topic: str, title: str, content: str) -> float:
        """ä½¿ç”¨NVIDIA GLM APIè¯„ä¼°ç›¸å…³æ€§"""
        if not settings.NVIDIA_API_KEY or settings.NVIDIA_API_KEY == "":
            return 0.5
        
        try:
            client = get_nvidia_client()
            if not client:
                return 0.5
            
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»ç›¸å…³æ€§è¯„ä¼°åŠ©æ‰‹ï¼Œæ“…é•¿è¯„ä¼°æ–°é—»ä¸ä¸»é¢˜çš„ç›¸å…³æ€§ã€‚"
            user_prompt = f"""ä¸»é¢˜ï¼š{topic}

æ–°é—»æ ‡é¢˜ï¼š{title}

æ–°é—»å†…å®¹ï¼š{content[:500]}

è¯·è¯„ä¼°è¿™æ¡æ–°é—»ä¸ä¸»é¢˜"{topic}"çš„ç›¸å…³æ€§ï¼Œç»™å‡º0-1ä¹‹é—´çš„åˆ†æ•°ï¼š
- 0.9-1.0: é«˜åº¦ç›¸å…³ï¼Œæ ¸å¿ƒå†…å®¹å®Œå…¨åŒ¹é…ä¸»é¢˜
- 0.7-0.9: è¾ƒä¸ºç›¸å…³ï¼Œä¸»è¦å†…å®¹ä¸ä¸»é¢˜ç›¸å…³
- 0.5-0.7: ä¸­ç­‰ç›¸å…³ï¼Œéƒ¨åˆ†å†…å®¹ä¸ä¸»é¢˜ç›¸å…³
- 0.3-0.5: ä½ç›¸å…³æ€§ï¼Œåªæœ‰å°‘é‡å†…å®¹ä¸ä¸»é¢˜ç›¸å…³
- 0.0-0.3: å‡ ä¹ä¸ç›¸å…³

è¯·åªè¿”å›ä¸€ä¸ª0-1ä¹‹é—´çš„æ•°å­—ï¼Œä¾‹å¦‚ï¼š0.85"""
            
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=50,
                stream=False
            )
            
            if response.choices and len(response.choices) > 0:
                message = response.choices[0].message
                content_text = message.content if message.content else ""
                if hasattr(message, 'reasoning_content') and message.reasoning_content:
                    content_text = message.reasoning_content
                
                # æå–æ•°å­—
                score_match = re.search(r'0?\.?\d+', content_text.strip())
                if score_match:
                    score = float(score_match.group())
                    # é™åˆ¶åœ¨0-1ä¹‹é—´
                    score = max(0.0, min(1.0, score))
                    logger.debug(f"Relevance score for '{title[:30]}...' with topic '{topic}': {score}")
                    return score
                else:
                    logger.warning(f"Could not parse relevance score from: {content_text}")
                    return 0.5
            else:
                return 0.5
                
        except Exception as e:
            logger.error(f"Error evaluating relevance with NVIDIA: {str(e)}")
            return 0.5
    
    def _evaluate_relevance_ollama(self, topic: str, title: str, content: str) -> float:
        """ä½¿ç”¨Ollamaè¯„ä¼°ç›¸å…³æ€§"""
        try:
            prompt = f"""ä¸»é¢˜ï¼š{topic}

æ–°é—»æ ‡é¢˜ï¼š{title}

æ–°é—»å†…å®¹ï¼š{content[:500]}

è¯·è¯„ä¼°è¿™æ¡æ–°é—»ä¸ä¸»é¢˜"{topic}"çš„ç›¸å…³æ€§ï¼Œç»™å‡º0-1ä¹‹é—´çš„åˆ†æ•°ï¼ˆ0å®Œå…¨ä¸ç›¸å…³ï¼Œ1å®Œå…¨ç›¸å…³ï¼‰ã€‚
åªè¿”å›ä¸€ä¸ªæ•°å­—ï¼Œä¾‹å¦‚ï¼š0.85"""
            
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "stream": False,
                "temperature": 0.3,
            }
            
            response = requests.post(self.api_url, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                content_text = ""
                if "message" in result and "content" in result["message"]:
                    content_text = result["message"]["content"].strip()
                elif "choices" in result and len(result["choices"]) > 0:
                    content_text = result["choices"][0]["message"]["content"].strip()
                
                # æå–æ•°å­—
                score_match = re.search(r'0?\.?\d+', content_text.strip())
                if score_match:
                    score = float(score_match.group())
                    score = max(0.0, min(1.0, score))
                    logger.debug(f"Relevance score (Ollama) for '{title[:30]}...' with topic '{topic}': {score}")
                    return score
                else:
                    logger.warning(f"Could not parse relevance score from Ollama response: {content_text}")
                    return 0.5
            else:
                return 0.5
                
        except Exception as e:
            logger.error(f"Error evaluating relevance with Ollama: {str(e)}")
            return 0.5
    
    def _evaluate_relevance_dashscope(self, topic: str, title: str, content: str) -> float:
        """ä½¿ç”¨DashScopeè¯„ä¼°ç›¸å…³æ€§"""
        if not settings.DASHSCOPE_API_KEY or settings.DASHSCOPE_API_KEY == "":
            return 0.5
        
        try:
            prompt = f"""ä¸»é¢˜ï¼š{topic}

æ–°é—»æ ‡é¢˜ï¼š{title}

æ–°é—»å†…å®¹ï¼š{content[:500]}

è¯·è¯„ä¼°è¿™æ¡æ–°é—»ä¸ä¸»é¢˜"{topic}"çš„ç›¸å…³æ€§ï¼Œç»™å‡º0-1ä¹‹é—´çš„åˆ†æ•°ï¼ˆ0å®Œå…¨ä¸ç›¸å…³ï¼Œ1å®Œå…¨ç›¸å…³ï¼‰ã€‚
åªè¿”å›ä¸€ä¸ªæ•°å­—ï¼Œä¾‹å¦‚ï¼š0.85"""
            
            response = dashscope.Generation.call(
                model=self.model,
                prompt=prompt,
                max_tokens=50,
                temperature=0.3,
                top_p=0.9
            )
            
            if response.status_code == 200:
                content_text = response.output.text.strip()
                # æå–æ•°å­—
                score_match = re.search(r'0?\.?\d+', content_text.strip())
                if score_match:
                    score = float(score_match.group())
                    score = max(0.0, min(1.0, score))
                    logger.debug(f"Relevance score (DashScope) for '{title[:30]}...' with topic '{topic}': {score}")
                    return score
                else:
                    logger.warning(f"Could not parse relevance score from DashScope response: {content_text}")
                    return 0.5
            else:
                return 0.5
                
        except Exception as e:
            logger.error(f"Error evaluating relevance with DashScope: {str(e)}")
            return 0.5
    
    def _fallback_summary(self, title: str, content: str, roast_mode: bool) -> str:
        """Fallback summary when API is not available"""
        # Simple truncation as fallback
        if content and len(content) > 100:
            summary = content[:100] + "..."
        elif content:
            summary = content
        else:
            summary = title
        
        if roast_mode:
            return f"ğŸ“° {summary} ï¼ˆAIæ‘˜è¦æš‚æ—¶ä¸å¯ç”¨ï¼‰"
        else:
            return summary
    
    def batch_summarize(self, articles: list, roast_mode: bool = False) -> list:
        """
        Batch process multiple articles
        
        Args:
            articles: List of dicts with 'title' and 'content' keys
            roast_mode: Whether to use roast mode
        
        Returns:
            List of articles with added 'summary' field
        """
        results = []
        
        for article in articles:
            try:
                summary = self.generate_summary(
                    article.get("title", ""),
                    article.get("content", ""),
                    roast_mode
                )
                article["summary"] = summary
                results.append(article)
            except Exception as e:
                logger.error(f"Batch summarize error: {str(e)}")
                article["summary"] = self._fallback_summary(
                    article.get("title", ""),
                    article.get("content", ""),
                    roast_mode
                )
                results.append(article)
        
        return results


# Singleton instance
_summarizer_instance = None

def get_summarizer() -> NewsSummarizer:
    """Get singleton summarizer instance"""
    global _summarizer_instance
    if _summarizer_instance is None:
        _summarizer_instance = NewsSummarizer()
    return _summarizer_instance
